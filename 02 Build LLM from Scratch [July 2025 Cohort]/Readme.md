## Lectures

- Lecture 1: Introduction to Course : 01:32:04
- Lecture 2: Introduction to LLM: Next token prediction : 01:36:37
- Lecture 3 – Pretaining + Finetuning : 01:32:16
- Lecture 4 – GPT for 5-6 year old kid, Tokenization : 01:41:59
- Lecture 5 – Word Based Tokenization : 01:29:32
- Lecture 6 – Sub Word Based Tokenization BPE : 01:35:50
- [Lecture 7 – BPE from Scratch](Lecture%207%20–%20BPE%20from%20Scratch.md)
- Lecture 8 – Input-Target Pair : 01:23:32
- Lecture 9 – Token Embedding : 01:28:53
- Lecture 10 – Position Embedding : 01:15:11

## Language Model Architecture

- Lecture 11 – Attention Mechanism : 01:24:37
- Lecture 12 – Self attention Query, Key, Value : 01:41:58

## Assignment

-
- Teaching Assignment 1: Explaining Large Language Models to a 15-Year-Old
- Assignment 2: Tokenization
-

# Notebooks

-
- Colab Book : Next token prediction probability
- Lecture 5 – Colab Notebook – Word Based Tokenization :
- Lecture 7 – BPE from Scratch – Colab Notebook :
- Lecture 10 – Till Input embedding – Colab Notebook :
-
