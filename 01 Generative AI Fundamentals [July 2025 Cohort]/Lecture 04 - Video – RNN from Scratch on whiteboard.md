# Lecture 04 - Video â€“ NN and RNN from Scratch

- https://zoom.us/clips/share/AJo7d35OSC65Cc1_TlsDlA

- **Sequence-to-Sequence Problems in Machine Translation**
  I discussed the concept of sequence-to-sequence problems, specifically focusing on machine translation from English to Hindi.

- **Neural Networks: Basics and Brain Parallels**
  I explained the basics of neural networks, drawing parallels to the human brain.

- **Linear Transformation Limitations in Function Modeling**
  I explained the concept of linear transformation and its limitations in modeling certain functions.

- **Activation Functions in Machine Learning Models**
  I discussed the use of activation functions in machine learning models.

- **Activation Functions in Machine Learning**
  I discussed the concept of activation functions in machine learning. I explained that activation functions are mathematical functions used in neural networks to introduce non-linearity into the model.

- **Activation Functions in Neural Networks**
  I explained the concept of activation functions in neural networks.

- **Challenges of Sequence-to-Sequence Learning**
  I explained the challenges of sequence-to-sequence learning using a simple example of a kindergarten student learning to predict the next word in a sequence.

- **Recurrent Neural Networks (RNNs) Explained**
  I explained the concept of Recurrent Neural Networks (RNNs) using a simple example.

- **Memory Propagation in Neural Networks**
  I explained the concept of a memory that propagates over time in a neural network. I used the example of a problem faced at point A and how the memory of solving that problem is used at point B.

- **Understanding Computers and Vocabulary Basics**
  I explained that computers only understand numbers and not words. I introduced the concept of vocabulary, which is a list of words or characters that a model can comprehend.

- **Understanding Model Inputs: Limitations of Encodings**
  I discussed the limitations of using encodings for understanding model inputs, emphasizing the need for contextual significance.
